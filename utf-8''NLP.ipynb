{
  "cells": [
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import string\nimport re\nimport numpy as np\nimport pandas as pd\nimport matplotlib\nfrom textblob import TextBlob\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.linear_model import SGDClassifier\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import SpatialDropout1D\nfrom keras.layers.embeddings import Embedding\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.callbacks import EarlyStopping\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\nmatplotlib.rc('xtick', labelsize=14)\nmatplotlib.rc('ytick', labelsize=14)\n",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "with open(\"full_set.txt\", 'r',encoding=\"utf8\") as f:\n    content = f.readlines()",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Preprocessing\n\n## Remove leading and trailing white space\ncontent = [x.strip() for x in content]\n## Separate the sentences from the labels\nsentences = [x.split(\"\\t\")[0] for x in content]\nlabels = [x.split(\"\\t\")[1] for x in content]\n\n## Transform the labels from '0 v.s. 1' to '-1 v.s. 1'\ny = np.array(labels, dtype='int8')\ny = 2*y - 1\n\ndef full_remove(x, removal_list):\n    for w in removal_list:\n        x = x.replace(w, ' ')\n    return x\n## Remove digits ##\ndigits = [str(x) for x in range(10)]\nremove_digits = [full_remove(x, digits) for x in sentences]\n## Remove punctuation ##\nremove_punc = [full_remove(x, list(string.punctuation)) for x in remove_digits]\n## Make everything lower-case and remove any white space ##\nsents_lower = [x.lower() for x in remove_punc]\nsents_lower = [x.strip() for x in sents_lower]\n\n\n## Remove stop words ##\nstop_set = ['the', 'a', 'an', 'i', 'he', 'she', 'they', 'to', 'of', 'it', 'from']\ndef removeStopWords(stop_set, txt):\n    newtxt = ' '.join([word for word in txt.split() if word not in stop_set])\n    return newtxt\nsents_processed = [removeStopWords(stop_set,x) for x in sents_lower]",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#bag of words creation\nvectorizer = CountVectorizer(analyzer = \"word\",\n                             preprocessor = None,\n                             stop_words =  'english',\n                             max_features = 6000, ngram_range=(1,5))\ndata_features = vectorizer.fit_transform(sents_processed)\ntfidf_transformer = TfidfTransformer()\ndata_features_tfidf = tfidf_transformer.fit_transform(data_features)\ndata_mat = data_features_tfidf.toarray()",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#data split\nnp.random.seed(0)\ntest_index = np.append(np.random.choice((np.where(y==-1))[0], 250, replace=False), np.random.choice((np.where(y==1))[0], 250, replace=False))\ntrain_index = list(set(range(len(labels))) - set(test_index))\ntrain_data = data_mat[train_index,]\ntrain_labels = y[train_index]\ntest_data = data_mat[test_index,]\ntest_labels = y[test_index]",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Create polarity function and subjectivity function\npol = lambda x: TextBlob(x).sentiment.polarity\nsub = lambda x: TextBlob(x).sentiment.subjectivity\npol_list = [pol(x) for x in sents_processed]\nsub_list = [sub(x) for x in sents_processed]\n\n## Fit logistic classifier on training data\nclf = SGDClassifier(loss=\"log\", penalty=\"none\")\nclf.fit(train_data, train_labels)\n## Pull out the parameters (w,b) of the logistic regression model\nw = clf.coef_[0,:]\nb = clf.intercept_\n## Get predictions on training and test data\npreds_train = clf.predict(train_data)\npreds_test = clf.predict(test_data)\n## Compute errors\nerrs_train = np.sum((preds_train > 0.0) != (train_labels > 0.0))\nerrs_test = np.sum((preds_test > 0.0) != (test_labels > 0.0))\nprint(\"Training error: \", float(errs_train)/len(train_labels))\nprint(\"Test error: \", float(errs_test)/len(test_labels))",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n  FutureWarning)\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "Training error:  0.0204\nTest error:  0.184\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "##LSTM network\nmax_review_length = 200\ntokenizer = Tokenizer(num_words=10000,  #max no. of unique words to keep\n                      filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~',\n                      lower=True #convert to lower case\n                     )\ntokenizer.fit_on_texts(sents_processed)\n\nX = tokenizer.texts_to_sequences(sents_processed)\nX = sequence.pad_sequences(X, maxlen= max_review_length)\n\nY=pd.get_dummies(y).values\n\nnp.random.seed(0)\ntest_inds = np.append(np.random.choice((np.where(y==-1))[0], 250, replace=False), np.random.choice((np.where(y==1))[0], 250, replace=False))\ntrain_inds = list(set(range(len(labels))) - set(test_inds))\ntrain_data = X[train_inds,]\ntrain_labels = Y[train_inds]\ntest_data = X[test_inds,]\ntest_labels = Y[test_inds]",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "##network Creation\nEMBEDDING_DIM = 200\nmodel = Sequential()\nmodel.add(Embedding(10000, EMBEDDING_DIM, input_length=X.shape[1]))\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(250, dropout=0.2,return_sequences=True))\nmodel.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nepochs = 2\nbatch_size = 40\nmodel.fit(train_data, train_labels,\n          epochs=epochs,\n          batch_size=batch_size,\n          validation_split=0.1,\n          verbose=0)",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_501/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:414: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": "<keras.callbacks.callbacks.History at 0x7f475431d3c8>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "###testing\noutcome_labels = ['Negative', 'Positive']\nnew = [\"Tresl is a great place to shop for loans\"]\n\nseq = tokenizer.texts_to_sequences(new)\npadded = sequence.pad_sequences(seq, maxlen=max_review_length)\npred = model.predict(padded)\nprint(\"Probability distribution: \", pred)\nprint(\"Is this a Positive or Negative review? \")\nprint(outcome_labels[np.argmax(pred)])",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Probability distribution:  [[0.0029238 0.9970763]]\nIs this a Positive or Negative review? \nPositive\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "## Convert vocabulary into a list:\nvocab = np.array([z[0] for z in sorted(vectorizer.vocabulary_.items(), key=lambda x:x[1])])\n## Get indices of sorting w\ninds = np.argsort(w)\n\n\n## Words with large negative values\nneg_inds = inds[0:50]\nprint(\"Highly negative words: \")\nprint([str(x) for x in list(vocab[neg_inds])])\n\n\n## Words with large positive values\npos_inds = inds[-49:-1]\nprint(\"Highly positive words: \")\nprint([str(x) for x in list(vocab[pos_inds])])\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Word Cloud\n\nfrom wordcloud import WordCloud\nwc = WordCloud(stopwords=stop_set, background_color=\"white\", colormap=\"Dark2\",\n               max_font_size=150, random_state=42)\nplt.rcParams['figure.figsize'] = [16, 6]\nwc.generate(\" \".join(list(vocab[neg_inds])))\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis(\"off\")#plt.show()",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}